{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Clustering Tool V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/searchsolved/Twittorials/blob/master/Keyword_Clustering_Tool/Keyword_Clustering_Tool_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xQe7XNSNyf"
      },
      "source": [
        "by [LeeFootSEO](https://twitter.com/LeeFootSEO) January 2022\n",
        "\n",
        "More like this: https://searchsolved.co.uk/blog/\n",
        "\n",
        "# Keyword Clustering Tool V3\n",
        "Automatic keyword clustering tool. See yours and your competitors most profitable keyword clusters in a couple of clicks.\n",
        "\n",
        "# Quick Start Instructions\n",
        "Run all the cells and upload a CSV export.\n",
        "Runtime > Run All (Control + F9)\n",
        "\n",
        "# Works with the Following Exports out the Box\n",
        "\n",
        "*   Ahrefs.com (Keyword Export / Site Explorer Export)\n",
        "*   SEMRush.com\n",
        "*   Search Console (Coverage Report CSV Export (Queries.csv))\n",
        "*   AdWords Search Terms Report .csv or Excel format (Beta)\n",
        "*   A simple single column .txt / csv file with keywords (Header or Headerless)\n",
        "\n",
        "# File Formats\n",
        "*   utf-8/utf-16/csv/xls/xlsx/xlsm/xlsb/odf/ods/odt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPAaQtNdMqlb",
        "outputId": "fda2a481-6e01-4494-87ff-404a5b676c15"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install polyfuzz[fast]\n",
        "!pip install chardet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: polyfuzz[fast] in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (0.11.2)\n",
            "Requirement already satisfied: joblib>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (4.62.3)\n",
            "Requirement already satisfied: rapidfuzz>=0.13.1 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (1.9.1)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (3.2.2)\n",
            "Requirement already satisfied: sparse-dot-topn>=0.2.9 in /usr/local/lib/python3.7/dist-packages (from polyfuzz[fast]) (0.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->polyfuzz[fast]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->polyfuzz[fast]) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->polyfuzz[fast]) (3.0.0)\n",
            "Requirement already satisfied: cython>=0.29.15 in /usr/local/lib/python3.7/dist-packages (from sparse-dot-topn>=0.2.9->polyfuzz[fast]) (0.29.24)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.7/dist-packages (from sparse-dot-topn>=0.2.9->polyfuzz[fast]) (57.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GPFNiTcMZnS"
      },
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "from google.colab import files\n",
        "from polyfuzz import PolyFuzz\n",
        "import chardet\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjknFtjMvwJ"
      },
      "source": [
        "# rename the parent cluster name using the keyword with the highest search volume (recommended)\n",
        "parent_by_vol = True\n",
        "drop_site_links = False\n",
        "drop_image_links = False\n",
        "sim_match_percent = 1\n",
        "url_filter = \"\"\n",
        "min_volume = 0  # set the minimum search volume / impressions to filter on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Custom Intent Classifiers\n",
        "You Can Add Custom Words to Classify Keywords with Below. e.g. 'vs' = Commercial Investigation, 'how' = 'informational' etc. \n",
        "\n",
        "Please see here: https://blog.travelpayouts.com/en/search-intent/ "
      ],
      "metadata": {
        "id": "K9BaiYKmOxD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info_filter = \"what|where|why|when|who|how|which|tip|guide|tutorial|ideas|example|learn|wiki|in mm|in cm|in ft|in feet\"\n",
        "comm_invest_filter = \"best|vs|list|compare|review|list|top|difference between\"\n",
        "trans_filter = \"purchase|bargain|cheap|deal|value|closeout|buy|shop|price|coupon|discount|price|pricing|delivery|shipping|order|returns|sale|amazon|target|ebay|walmart|cost of|how much\""
      ],
      "metadata": {
        "id": "sMlwHf9dOwsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Your Keyword File Here!"
      ],
      "metadata": {
        "id": "WvAeQM8bQzOG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oA_ejVsPrgo"
      },
      "source": [
        "# upload the keyword export\n",
        "upload = files.upload()\n",
        "input_file = list(upload.keys())[0]  # get the name of the uploaded file\n",
        "# test the file extension\n",
        "file_extension = os.path.splitext(input_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ItHhRwErGkm"
      },
      "source": [
        "# ---------------------------------- auto detect character encoding ----------------------------------------------------\n",
        "\n",
        "with open(input_file, 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(10000))\n",
        "\n",
        "# if the encoding is utf-16 use a space separator, else ','\n",
        "if result['encoding'] == \"UTF-16\":\n",
        "    white_space = True\n",
        "else:\n",
        "    white_space = False\n",
        "\n",
        "if (\n",
        "    file_extension[1] == \".xlsx\"\n",
        "    or file_extension[1] == \".xls\"\n",
        "    or file_extension[1] == \".xlsm\"\n",
        "    or file_extension[1] == \".xlsb\"\n",
        "    or file_extension[1] == \".odf\"\n",
        "    or file_extension[1] == \".ods\"\n",
        "    or file_extension[1] == \".odt\"\n",
        "):\n",
        "    df_1 = pd.read_excel(input_file, engine=\"openpyxl\")\n",
        "else:\n",
        "    try:\n",
        "        df_1 = pd.read_csv(\n",
        "            input_file,\n",
        "            encoding=result[\"encoding\"],\n",
        "            delim_whitespace=white_space,\n",
        "            error_bad_lines=False,\n",
        "        )\n",
        "    # fall back to utf-8\n",
        "    except UnicodeDecodeError:\n",
        "        df_1 = pd.read_csv(\n",
        "            input_file,\n",
        "            encoding=\"utf-8\",\n",
        "            delim_whitespace=white_space,\n",
        "            error_bad_lines=False,\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhk1iewZrH0f"
      },
      "source": [
        "# -------------------------- check if single column import / and write header if missing -------------------------------\n",
        "\n",
        "# check the number of columns\n",
        "col_len = len(df_1.columns)\n",
        "col_name = df_1.columns[0]\n",
        "\n",
        "if col_len == 1 and df_1.columns[0] != \"Keyword\":\n",
        "    df_1.columns = [\"Keyword\"]\n",
        "\n",
        "if col_len == 1 and df_1.columns[0] != \"keyword\":\n",
        "    df_1.columns = [\"Keyword\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBlgRin3rJw9"
      },
      "source": [
        "# -------------------------- detect if import file is adwords and remove the first two rows ----------------------------\n",
        "adwords_check = False\n",
        "if col_name == \"Search terms report\":\n",
        "    df_1.columns = df_1.iloc[1]\n",
        "    df_1 = df_1[1:]\n",
        "    df_1 = df_1.reset_index(drop=True)\n",
        "\n",
        "    new_header = df_1.iloc[0]  # grab the first row for the header\n",
        "    df_1 = df_1[1:]  # take the data less the header row\n",
        "    df_1.columns = new_header  # set the header row as the df header\n",
        "    adwords_check = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv0xmvs7rNwn"
      },
      "source": [
        "# --------------------------------- Check if csv data is gsc and set bool ----------------------------------------------\n",
        "\n",
        "if 'Impressions' in df_1.columns:\n",
        "    gsc_data = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPQa5Z-qrQFH"
      },
      "source": [
        "# ----------------- standardise the column names between ahrefs v1/v2/semrush/gsc keyword exports ----------------------\n",
        "\n",
        "df_1.rename(\n",
        "    columns={\n",
        "        \"Current position\": \"Position\",\n",
        "        \"Current URL\": \"URL\",\n",
        "        \"Current URL inside\": \"Page URL inside\",\n",
        "        \"Current traffic\": \"Traffic\",\n",
        "        \"KD\": \"Difficulty\",\n",
        "        \"Keyword Difficulty\": \"Difficulty\",\n",
        "        \"Search Volume\": \"Volume\",\n",
        "        \"page\": \"URL\",\n",
        "        \"query\": \"Keyword\",\n",
        "        \"Top queries\": \"Keyword\",\n",
        "        \"Impressions\": \"Volume\",\n",
        "        \"Clicks\": \"Traffic\",\n",
        "        \"Search term\": \"Keyword\",\n",
        "        \"Impr.\": \"Volume\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGJtWEKOrQvm"
      },
      "source": [
        "# ------------------------------ check number of imported rows and warn if excessive -----------------------------------\n",
        "\n",
        "row_len = len(df_1)\n",
        "if col_len > 1:\n",
        "    # --------------------------------- clean the data pre-grouping ----------------------------------------------------\n",
        "\n",
        "    if url_filter:\n",
        "        print(\"Processing only URLs containing:\", url_filter)\n",
        "\n",
        "    try:\n",
        "        df_1 = df_1[df_1[\"URL\"].str.contains(url_filter, na=False)]\n",
        "    except KeyError:\n",
        "        pass\n",
        "\n",
        "    # ========================= clean strings out of numerical columns (adwords) ========================================\n",
        "\n",
        "    try:\n",
        "        df_1[\"Volume\"] = df_1[\"Volume\"].str.replace(\",\", \"\").astype(int)\n",
        "        df_1[\"Traffic\"] = df_1[\"Traffic\"].str.replace(\",\", \"\").astype(int)\n",
        "        df_1[\"Conv. value / click\"] = df_1[\"Conv. value / click\"].str.replace(\",\", \"\").astype(float)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].str.replace(\",\", \"\").astype(float)\n",
        "        df_1[\"CTR\"] = df_1[\"CTR\"].replace(\" --\", \"0\", regex=True)\n",
        "        df_1[\"CTR\"] = df_1[\"CTR\"].str.replace(\"\\%\", \"\").astype(float)\n",
        "        df_1[\"Cost\"] = df_1[\"Cost\"].astype(float)\n",
        "        df_1[\"Conversions\"] = df_1[\"Conversions\"].astype(int)\n",
        "        df_1[\"Cost\"] = df_1[\"Cost\"].round(2)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].astype(float)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].round(2)\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(\"Total: \", na=False)]  # remove totals rows\n",
        "    df_1 = df_1[df_1[\"Keyword\"].notna()]  # keep only rows which are NaN\n",
        "    df_1 = df_1[df_1[\"Volume\"].notna()]  # keep only rows which are NaN\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].astype(str)\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].apply(lambda x: x.replace(\"0-10\", \"0\"))\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].astype(float).astype(int)\n",
        "\n",
        "    # drop sitelinks\n",
        "\n",
        "    if drop_site_links:\n",
        "        try:\n",
        "            df_1 = df_1[~df_1[\"Page URL inside\"].str.contains(\"Sitelinks\", na=False)]  # drop sitelinks\n",
        "        except KeyError:\n",
        "            pass\n",
        "        try:\n",
        "            if gsc_data:\n",
        "                df_1 = df_1.sort_values(by=\"Traffic\", ascending=False)\n",
        "                df_1.drop_duplicates(subset=\"Keyword\", keep=\"first\", inplace=True)\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "    if drop_image_links:\n",
        "        try:\n",
        "            df_1 = df_1[~df_1[\"Page URL inside\"].str.contains(\"Image pack\", na=False)]  # drop image pack\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    df_1 = df_1[df_1[\"Volume\"] > min_volume]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEptMghirbtA"
      },
      "source": [
        "# ------------------------------------- do the grouping ----------------------------------------------------------------\n",
        "\n",
        "df_1_list = df_1.Keyword.tolist()  # create list from df\n",
        "model = PolyFuzz(\"TF-IDF\")\n",
        "\n",
        "cluster_tags = df_1_list[::]\n",
        "cluster_tags = set(cluster_tags)\n",
        "cluster_tags = list(cluster_tags)\n",
        "\n",
        "try:\n",
        "    model.match(df_1_list, cluster_tags)\n",
        "except ValueError:\n",
        "    print(\"Empty Dataframe, Can't Match - Check the URL Filter!\")\n",
        "    sys.exit()\n",
        "\n",
        "model.group(link_min_similarity=sim_match_percent)\n",
        "df_matched = model.get_matches()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlD59vIsreG2"
      },
      "source": [
        "# ------------------------------- clean the data post-grouping ---------------------------------------------------------\n",
        "\n",
        "df_matched.rename(columns={\"From\": \"Keyword\", \"Group\": \"Cluster Name\"}, inplace=True)  # renaming multiple columns\n",
        "\n",
        "# merge keyword volume / CPC / Pos / URL etc data from original dataframe back in\n",
        "df_matched = pd.merge(df_matched, df_1, on=\"Keyword\", how=\"left\")\n",
        "\n",
        "# rename traffic (acs) / (desc) to 'Traffic for standardisation\n",
        "df_matched.rename(columns={\"Traffic (desc)\": \"Traffic\", \"Traffic (asc)\": \"Traffic\"}, inplace=True)\n",
        "\n",
        "if col_len > 1:\n",
        "\n",
        "    # fill in missing values\n",
        "    df_matched.fillna({\"Traffic\": 0, \"CPC\": 0}, inplace=True)\n",
        "    df_matched['Traffic'] = df_matched['Traffic'].round(0)\n",
        "    # ------------------------- group the data and merge in original stats -------------------------------------------------\n",
        "    if not adwords_check:\n",
        "        try:\n",
        "            # make dedicated grouped dataframe\n",
        "            df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "                {\"Volume\": sum, \"Difficulty\": \"median\", \"CPC\": \"median\", \"Traffic\": sum}).reset_index())\n",
        "        except Exception:\n",
        "            df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "                {\"Volume\": sum, \"Traffic\": sum}).reset_index())\n",
        "\n",
        "        df_grouped = df_grouped.rename(\n",
        "            columns={\"Volume\": \"Cluster Volume\", \"Difficulty\": \"Cluster KD (Median)\", \"CPC\": \"Cluster CPC (Median)\",\n",
        "                     \"Traffic\": \"Cluster Traffic\"})\n",
        "\n",
        "        df_matched = pd.merge(df_matched, df_grouped, on=\"Cluster Name\", how=\"left\")  # merge in the group stats\n",
        "\n",
        "    if adwords_check:\n",
        "\n",
        "        df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "            {\"Volume\": sum, \"CTR\": \"median\", \"Cost\": sum, \"Traffic\": sum, \"All conv. value\": sum, \"Conversions\": sum}).reset_index())\n",
        "\n",
        "        df_grouped = df_grouped.rename(\n",
        "            columns={\"Volume\": \"Cluster Volume\", \"CTR\": \"Cluster CTR (Median)\", \"Cost\": \"Cluster Cost (Sum)\",\n",
        "                     \"Traffic\": \"Cluster Traffic\", \"All conv. value\": \"All conv. value (Sum)\", \"Conversions\": \"Cluster Conversions (Sum)\"})\n",
        "\n",
        "        df_matched = pd.merge(df_matched, df_grouped, on=\"Cluster Name\", how=\"left\")  # merge in the group stats\n",
        "\n",
        "        del df_matched['To']\n",
        "        del df_matched['Similarity']\n",
        "\n",
        "    # ---------------------------- clean and sort the final output -----------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        df_matched.drop_duplicates(subset=[\"URL\", \"Keyword\"], keep=\"first\", inplace=True)  # drop if both kw & url are duped\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Svmxokri1O"
      },
      "source": [
        "if not adwords_check:\n",
        "    cols = (\n",
        "        \"Cluster Name\",\n",
        "        \"Keyword\",\n",
        "        \"Cluster Size\",\n",
        "        \"Cluster Volume\",\n",
        "        \"Cluster KD (Median)\",\n",
        "        \"Cluster CPC (Median)\",\n",
        "        \"Cluster Traffic\",\n",
        "        \"Volume\",\n",
        "        \"Difficulty\",\n",
        "        \"CPC\",\n",
        "        \"Traffic\",\n",
        "        \"URL\",\n",
        "    )\n",
        "\n",
        "    df_matched = df_matched.reindex(columns=cols)\n",
        "\n",
        "    try:\n",
        "        if gsc_data:\n",
        "            cols = \"Cluster Name\", \"Keyword\", \"Cluster Size\", \"Cluster Volume\", \"Cluster Traffic\", \"Volume\", \"Traffic\"\n",
        "            df_matched = df_matched.reindex(columns=cols)\n",
        "    except NameError:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib69NIuwrj2-"
      },
      "source": [
        "# ------------ get the keyword with the highest search volume to replace the auto generated tag name with --------------\n",
        "\n",
        "if col_len > 1:\n",
        "    if parent_by_vol:\n",
        "        df_matched['vol_max'] = df_matched.groupby(['Cluster Name'])['Volume'].transform(max)\n",
        "        # this sort is mandatory for the renaming to work properly by floating highest values to the top of the cluster\n",
        "        df_matched.sort_values([\"Cluster Name\", \"Cluster Volume\", \"Volume\"], ascending=[False, True, False], inplace=True)\n",
        "        df_matched['exact_vol_match'] = df_matched['vol_max'] == df_matched['Volume']\n",
        "        df_matched.loc[df_matched['exact_vol_match'] == True, 'highest_ranked_keyword'] = df_matched['Keyword']\n",
        "        df_matched['highest_ranked_keyword'] = df_matched['highest_ranked_keyword'].fillna(method='ffill')\n",
        "        df_matched['Cluster Name'] = df_matched['highest_ranked_keyword']\n",
        "        del df_matched['vol_max']\n",
        "        del df_matched['exact_vol_match']\n",
        "        del df_matched['highest_ranked_keyword']\n",
        "if adwords_check:\n",
        "    df_matched = df_matched.rename(columns={\"Volume\": \"Impressions\", \"Traffic\": \"Clicks\", \"Cluster Traffic\": \"Cluster Clicks (Sum)\"})\n",
        "  \n",
        "# count cluster_size\n",
        "df_matched['Cluster Size'] = df_matched['Cluster Name'].map(df_matched.groupby('Cluster Name')['Cluster Name'].count())\n",
        "df_matched.loc[df_matched[\"Cluster Size\"] == 1, \"Cluster Name\"] = \"no_cluster_available\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe76sl5Vror2"
      },
      "source": [
        "# -------------------------------------- final output ------------------------------------------------------------------\n",
        "# sort on cluster size\n",
        "df_matched.sort_values([\"Cluster Size\", \"Cluster Name\", \"Cluster Volume\"], ascending=[False, True, False], inplace=True)\n",
        "\n",
        "try:\n",
        "    if gsc_data:\n",
        "        df_matched.rename(\n",
        "            columns={\"Cluster Volume\": \"Cluster Impressions\", \"Cluster Traffic\": \"Cluster Clicks\", \"Traffic\": \"Clicks\",\n",
        "                     \"Volume\": \"Impressions\"}, inplace=True)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "if col_len == 1:\n",
        "    cols = \"Cluster Name\", \"Keyword\", \"Cluster Size\"\n",
        "    df_matched = df_matched.reindex(columns=cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_matched)"
      ],
      "metadata": {
        "id": "4QFzhJmHXIPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - add in intent markers\n",
        "colname = df_matched.columns[1]\n",
        "df_matched.loc[df_matched[colname].str.contains(info_filter), \"Informational\"] = \"Informational\"\n",
        "df_matched.loc[df_matched[colname].str.contains(comm_invest_filter), \"Commercial Investigation\"] = \"Commercial Investigation\"\n",
        "df_matched.loc[df_matched[colname].str.contains(trans_filter), \"Transactional\"] = \"Transactional\""
      ],
      "metadata": {
        "id": "_EvJIgxWOdaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_matched.to_csv('your_keywords_clustered.csv', index=False)\n",
        "files.download(\"your_keywords_clustered.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "zoLLPJlxOf_3",
        "outputId": "f738db8a-2697-4561-9684-3cc739faf2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_85ac8ba9-93ae-4a51-9998-b648b1bcfcd3\", \"your_keywords_clustered.csv\", 6733907)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}